#!/bin/bash

#SBATCH --job-name=offline-simpo-8b
#SBATCH --account=def-afyshe-ab
#SBATCH --nodes=1
#SBATCH --gpus-per-node=4
#SBATCH --cpus-per-gpu=8
#SBATCH --mem=128G
#SBATCH --time=09:00:00
#SBATCH --open-mode=append
#SBATCH --wait-all-nodes=1
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

module --force purge

eval "$(conda shell.bash hook)"
conda activate llm-env

# For reading key=value arguments
for ARGUMENT in "$@"
do
	KEY=$(echo $ARGUMENT | cut -f1 -d=)
	KEY_LENGTH=${#KEY}
	VALUE="${ARGUMENT:$KEY_LENGTH+1}"
	export "$KEY"="$VALUE"
done

export CUDA_HOME=$CONDA_PREFIX
export NCCL_HOME=$CONDA_PREFIX
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib

date;pwd

# export MASTER_ADDR="$(hostname --fqdn)"
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT="$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1])')"
echo "RDZV Endpoint $MASTER_ADDR:$MASTER_PORT"


avg_logps=${AVG_LOGPS}
lr=${LR}
beta=${BETA}
output_dir=${OUTPUT_DIR}
gamma_to_beta=${GAMMA_TO_BETA}

RUN_NAME="llama3-8b-offline-simpo-beta_${beta}-lr_${lr}-gamma-to-beta_${gamma_to_beta}-avg_logps_${avg_logps}-v1"

# export TORCH_DISTRIBUTED_DEBUG=DETAIL
# export NCCL_DEBUG=TRACE
# export NCCL_DEBUG_SUBSYS=WARN
# export TORCH_CPP_LOG_LEVEL=INFO
# export LOGLEVEL=INFO
export NCCL_ASYNC_ERROR_HANDLING=1

# # No infinity band on the vulcan cluster.
# export NCCL_IB_DISABLE=1
# export NCCL_SOCKET_IFNAME=eth0
export WANDB_PROJECT="llama3-8b-v1"
export WANDB_MODE="offline"
export ACCELERATE_LOG_LEVEL="info"

LOG_DIR="llama3-8b-v1_training_logs"
LOG_PATH="${LOG_DIR}/log_${SLURM_JOB_ID}_rank_${SLURM_PROCID}_run_${RUN_NAME}.log"
# Make logging directories.
mkdir -p "${LOG_DIR}"

echo "Placing logs in: ${LOG_DIR}"
echo "Number of nodes: ${SLURM_NNODES}"

nvidia-smi

NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

TOTAL_GPUS=$((NUM_GPUs*SLURM_NNODES))

echo $SLURM_NODEID

accelerate launch \
    --config_file=recipes/accelerate_configs/deepspeed_zero2.yaml \
    --num_machines $SLURM_NNODES \
    --num_processes $TOTAL_GPUS \
    --main_process_ip $MASTER_ADDR \
    --main_process_port $MASTER_PORT \
    --machine_rank 0 \
    --rdzv_conf "rdzv_backend=c10d,rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT" \
    scripts/run_dpo.py recipes/llama-3-8b/simpo/config_qlora.vulcan.yaml \
        --average_logps=${avg_logps} \
        --learning_rate=${lr} \
        --beta=${beta} \
        --gamma_beta_ratio=${gamma_to_beta} \
        --output_dir=${output_dir}/${RUN_NAME} \
        --run_name=${RUN_NAME} > ${LOG_PATH} 2>&1


