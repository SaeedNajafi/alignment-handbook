#!/bin/bash

#SBATCH --job-name=sft-smollm
#SBATCH --account=def-afyshe-ab
#SBATCH --nodes=1
#SBATCH --gpus-per-node=4
#SBATCH --cpus-per-gpu=8
#SBATCH --mem=128G
#SBATCH --time=48:00:00
#SBATCH --open-mode=append
#SBATCH --wait-all-nodes=1
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err


module --force purge

eval "$(conda shell.bash hook)"
conda activate llm-env

# model_name_or_path=HuggingFaceTB/SmolLM2-135M
# tokenizer_name_or_path=HuggingFaceTB/SmolLM2-135M-Instruct
# run_name=smollm2_135M_sft
# output_dir=/home/saeednjf/projects/def-afyshe-ab/saeednjf/checkpoints/smollm2/${run_name}

# model_name_or_path=HuggingFaceTB/SmolLM2-360M
# tokenizer_name_or_path=HuggingFaceTB/SmolLM2-360M-Instruct
# run_name=smollm2_360M_sft
# output_dir=/home/saeednjf/projects/def-afyshe-ab/saeednjf/checkpoints/smollm2/${run_name}

model_name_or_path=HuggingFaceTB/SmolLM2-1.7B
tokenizer_name_or_path=HuggingFaceTB/SmolLM2-1.7B-Instruct
run_name=smollm2_1.7B_sft
output_dir=/home/saeednjf/projects/def-afyshe-ab/saeednjf/checkpoints/smollm2/${run_name}

hub_model_id=${run_name}

export CUDA_HOME=$CONDA_PREFIX
export NCCL_HOME=$CONDA_PREFIX
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib

date;pwd

export MASTER_ADDR="$(hostname --fqdn)"
export MASTER_PORT="$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1])')"
export RDVZ_ID=$RANDOM
echo "RDZV Endpoint $MASTER_ADDR:$MASTER_PORT"

LOG_DIR="sft_smollm_training_logs"
LOG_PATH="${LOG_DIR}/log_${SLURM_JOB_ID}_rank_${SLURM_PROCID}.log"
# Make logging directories.
mkdir -p "${LOG_DIR}"

echo "Placing logs in: ${LOG_DIR}"
echo "Number of nodes: ${SLURM_NNODES}"

nvidia-smi

NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

export NCCL_ASYNC_ERROR_HANDLING=1
export WANDB_PROJECT="smollm-experiments"
export WANDB_MODE="offline"
export ACCELERATE_LOG_LEVEL="info"


NUM_PROCS=$((NUM_GPUs*SLURM_NNODES))

accelerate launch \
    --config_file=recipes/accelerate_configs/deepspeed_zero2.yaml \
    --num_machines $SLURM_NNODES \
    --num_processes 4 \
    --main_process_ip $MASTER_ADDR \
    --main_process_port $MASTER_PORT \
    --machine_rank 0 \
    --rdzv_conf "rdzv_backend=c10d,rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT" \
    scripts/run_sft.py recipes/smollm2/sft/config_smol.yaml \
        --model_name_or_path=${model_name_or_path} \
        --tokenizer_name_or_path=${tokenizer_name_or_path} \
        --output_dir=${output_dir} \
        --run_name=${run_name} \
        --hub_model_id=${hub_model_id} > ${LOG_PATH} 2>&1


